  # NAO-Speaks-usingLLMS
This is a simple  program to add NLP and LLM through ( optional Deepseek-8b and locally run RAGS )  currently using ollama for simplicity (recommended llama.cpp for CPU Offloading.

## ğŸ† Project Overview
Welcome to **Interactive NAO **â€” This enables the **NAO robot** to replicate human interactions to a degree **speech recognition and LLMS**.

This project integrates:
- ğŸ¤ **Speech Recognition** using `speech_recognition`.
- ğŸ¤– **Natural Language Processing** via `DeepSeek-R1-Distill-llama-8B`. this is optional you can also use any model i tried it with a 1.5b model with not so good result.
- ğŸ­ **Choreographed Movements** controlled through dynamic commands.
- ğŸ— **Ollama Backend** for lightweight execution. (It's easier for this purpose than say llama.cpp but  i will recomment looking into something else for CPU ofloading)

![NAO Robot in Action](image1.png)
 
---

## ğŸ› ï¸ Installation Guide (using CLI)

### 1ï¸âƒ£ Install Dependencies
```sh
pip install speechrecognition  pyaudio ollama torch naoqi  
```

### 2ï¸âƒ£ Pull the AI Model (DeepSeek or Custom)
```sh
ollama pull your-model-name
```


## ğŸ™ï¸ How It Works
 The system listens for a **voice command** (or text input).  
 If itâ€™s a **motion command** (e.g., `dance`, `sit`), NAO executes it.  
 Otherwise, the system **responds using **.  
 Recognized text is processed by **DeepSeek AI** via `ollama.chat()`.  
